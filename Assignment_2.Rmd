---
documentclass: article
fontsize: 10pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    fig_caption: yes
    toc: false
    latex_engine: xelatex
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}

update.packages("rlang")
library(xfun)
library(class)#for the knn.cv function
library(tidyverse)
library(rprojroot)
library(kableExtra)
library(MASS)#lda function
library(biotools)
library(ISLR2)
library(patchwork)
library(randomForest) 
library(viridis)
# this package does not work for the newest version of R --> 
# only load if the local data files are not there
if (!file.exists(find_root_file("data/rect_constr.csv", 
     criterion = has_file("MultivariateStatistics_assignment.Rproj")))) {
  library(smacof)
} 
 
knitr::opts_chunk$set(echo = TRUE)
```

# Task 1

All R scripts and the data can be found on [this GitHub repository](https://github.com/raiisac/MultivariateStatistics_assignment).
```{r load, include=FALSE, message=FALSE, warning=FALSE}
#load the data and functions
data(College)

#test the difference between centroids
Wilks_manova <- function(data, group = "Private"){
  lm.out <- lm(cbind(
    sapply(X = colnames(data)[colnames(data) != group], 
           FUN = function(x) get(x)))~ 
       get((group)), 
   data = data)
  s <- summary(manova(lm.out), test = c("Wilks"))
  return(s$stats[1, "Pr(>F)"])#return the pvalue
}
#test the differences in var-cov matrix between groups
boxM_test <- function(data, group = "Private"){
  out <- boxM(data[,colnames(data) != group], data[,group])
  return(out$p.value)#return the pvalue
}

#execute lda with LOOCV and save the hit rate, sensitivity, and specificity
lda_analysis <- function(data, group = "Private", datatype){
  K <- length(unique(levels(data[,group])))
  positivelevel <- levels(data[,group])[K]# by default, it is assumed the last level is the positive level
  negativelevel <-  levels(data[,group])[-K]
  lda.out <- lda(
    formula(paste0(group, "~", paste(colnames(data)[colnames(data) != group], 
                                     collapse = "+"))),
    data = data,
    prior = rep(1/K, K),
    CV = TRUE)
  tab <- table(data[,group], lda.out$class)
  out <- data.frame(hitrate = sum(diag(tab))/sum(tab),
                    sensitivity = tab[positivelevel, positivelevel] /
                      sum(tab[positivelevel,]),
                    specificity = tab[negativelevel, negativelevel] /
                      sum(tab[negativelevel,]),
                    data = datatype,
                    method = "LDA"
  )
  return(out)
}
#execute qda with LOOCV and save the hit rate, sensitivity, and specificity
qda_analysis <- function(data, group = "Private", datatype){
  K <- length(unique(levels(data[,group])))
  positivelevel <- levels(data[,group])[K]# by default, it is assumed the last level is the positive level
  negativelevel <-  levels(data[,group])[-K]
  qda.out <- qda(
    formula(paste0(group, "~", paste(colnames(data)[colnames(data) != group], 
                                     collapse = "+"))),
    data = data,
    prior = rep(1/K, K),
    CV = TRUE)
  tab <- table(data[,group], qda.out$class)
  out <- data.frame(hitrate = sum(diag(tab))/sum(tab),
                    sensitivity = tab[positivelevel, positivelevel] /
                      sum(tab[positivelevel,]),
                    specificity = tab[negativelevel, negativelevel] /
                      sum(tab[negativelevel,]),
                    data = datatype,
                    method = "QDA"
  )
  return(out)
}

knn_analysis <-  function(data, group = "Private", datatype, testk){
  #In this function, we test a knn approach with several different values of K and we choose the one with the highest hit rate. If there are ties for the best hit rate, we choose the lowest K
  #testk is a vector for all values of K that should be tried.
  K <- length(unique(levels(data[,group])))
  positivelevel <- levels(data[,group])[K]# by default, it is assumed the last level is the positive level
  negativelevel <-  levels(data[,group])[-K]
  output <- data.frame(hitrate = NA,
                    sensitivity = NA,
                    specificity = NA,
                    data = datatype,
                    method = "KNN",
                    K = testk)
  for (x in testk) {
    knn.out <- knn.cv(train = data[,colnames(data) != group],
                      cl = data[, group],
                      k = x,
                      prob = TRUE
                      )
    tab <- table(data[,group], unname(knn.out))
    out <- data.frame(hitrate = sum(diag(tab))/sum(tab),
                    sensitivity = tab[positivelevel, positivelevel] /
                      sum(tab[positivelevel,]),
                    specificity = tab[negativelevel, negativelevel] /
                      sum(tab[negativelevel,]),
                    data = datatype,
                    method = "KNN",
                    K = x
    )
  output[x, ] <- out
  }
  graph <- output %>%
    pivot_longer(cols = c("hitrate","sensitivity", "specificity"),
                 names_to = "performance.measure", values_to = "value") %>%
    ggplot() +
    geom_line(aes(color = `performance.measure`, x = K, y = value)) +
    ylab("Performance") + theme_bw() + ylim(0.6, 1)
  selected <- which(output$hitrate == max(output$hitrate))[1]
  return(list(graph,output[selected,]))
}

bagging_analysis <- function(data, group = "Private", datatype, ntree){
  K <- length(unique(levels(data[,group])))
  positivelevel <- levels(data[,group])[K]# by default, it is assumed the last level is the positive level
  negativelevel <-  levels(data[,group])[-K]
  bagging.out <- randomForest(
    formula(paste0(group, "~", paste(colnames(data)[colnames(data) != group], 
                                     collapse = "+"))),
    data = data,
    mtry = K,
    ntree = ntree,
    importance = TRUE)
  tab <- table(data[,group], bagging.out$predicted) #the predicted values of the input data based on out-of-bag samples.
  out <- data.frame(hitrate = sum(diag(tab))/sum(tab),
                    sensitivity = tab[positivelevel, positivelevel] /
                      sum(tab[positivelevel,]),
                    specificity = tab[negativelevel, negativelevel] /
                      sum(tab[negativelevel,]),
                    data = datatype,
                    method = "bagging"
    )
  graph <- bagging.out$importance %>% 
    as.data.frame %>%
    dplyr::select(MeanDecreaseGini) %>%
    mutate(Var = rownames( bagging.out$importance )) %>%
    ggplot() + geom_point(aes(x=MeanDecreaseGini, y=Var)) + 
  scale_y_discrete(limits = names(sort(bagging.out$importance[,4]))) +
    theme_bw() + ylab("")
  return(list(graph, out))
}

RF_analysis <- function(data, group = "Private", datatype, ntree, mtry){
  K <- length(unique(levels(data[,group])))
  positivelevel <- levels(data[,group])[K]# by default, it is assumed the last level is the positive level
  negativelevel <-  levels(data[,group])[-K]
  bagging.out <- randomForest(
    formula(paste0(group, "~", paste(colnames(data)[colnames(data) != group], 
                                     collapse = "+"))),
    data = data,
    mtry = mtry,
    ntree = ntree,
    importance = TRUE)
  tab <- table(data[,group], bagging.out$predicted) #the predicted values of the input data based on out-of-bag samples.
  out <- data.frame(hitrate = sum(diag(tab))/sum(tab),
                    sensitivity = tab[positivelevel, positivelevel] /
                      sum(tab[positivelevel,]),
                    specificity = tab[negativelevel, negativelevel] /
                      sum(tab[negativelevel,]),
                    data = datatype,
                    method = "randomForest"
    )
  graph <- bagging.out$importance %>% 
    as.data.frame %>%
    dplyr::select(MeanDecreaseGini) %>%
    mutate(Var = rownames( bagging.out$importance )) %>%
    ggplot() + geom_point(aes(x=MeanDecreaseGini, y=Var)) + 
  scale_y_discrete(limits = names(sort(bagging.out$importance[,4]))) +
    theme_bw() + ylab("")
  return(list(graph, out))
}

```

The data consists of `r ncol(College)` variables of which the categorical variable *Private* denotes whether the school is private (*Private == Yes*) or not (*Private == No*). There are `r nrow(College)` schools in the data; `r sum(College$Private == "Yes")` (`r round(sum(College$Private == "Yes")/nrow(College)*100,2)`%) private schools and `r sum(College$Private == "No")` (`r round(sum(College$Private == "No")/nrow(College)*100,2)`%) non-private schools.

Before we start to build models to distinguish private and non-private schools, Wilk's lambda test is performed where the null hypothesis states that the centroid (mean) in both groups is the same. Both in the log-transformed and the original (centered or standardized) data, this null hypothesis is rejected. This means that the centroids of private and non-private schools differ significantly. 

The test of Box to test for equality of within-group covariance matrices shows that $H_0$ of equal covariance matrices across groups is not supported by the data. However, we still report on the results from LDA (which assumes equal covariances) since the alternative, QDA, does not always perform better because lower bias of the QDA classifier mat not outweigh its higher model complexity.

```{r classification, include=TRUE, echo = TRUE, message=FALSE, warning=FALSE}
College_logtranfskewed <- College %>%
  mutate(Apps = log(Apps),
         Accept = log(Accept),
         Enroll = log(Enroll),
         Top10perc = log(Top10perc),
         F.Undergrad = log(F.Undergrad),
         P.Undergrad = log(P.Undergrad),
         Books = log(Books),
         Personal = log(Personal),
         Expend = log(Expend))
College_centered <- College %>% 
  dplyr::select(-Private) %>% 
  scale(center = TRUE, scale = FALSE) %>%
  as.data.frame() %>%
  mutate(Private = College$Private)
College_logtranfskewed_centered <- College_logtranfskewed %>% 
  dplyr::select(-Private) %>%  
  scale(center = TRUE, scale = FALSE) %>%
  as.data.frame() %>%
  mutate(Private = College$Private)
College_std <- College %>% 
  dplyr::select(-Private) %>%  
  scale(center = TRUE, scale = TRUE) %>%
  as.data.frame() %>%
  mutate(Private = College$Private)
College_logtranfskewed_std <- College_logtranfskewed %>% 
  dplyr::select(-Private) %>%  
  scale(center = TRUE, scale = TRUE) %>%
  as.data.frame() %>%
  mutate(Private = College$Private)
testlist <- list(center = College_centered,
                 center_logtransfskewed = College_logtranfskewed_centered,
                 std = College_std,
                 std_logtransfskewed = College_logtranfskewed_std
                 )

#test the difference between centroids
wilks_results <- testlist %>% map(function(x) Wilks_manova(x, "Private"))
#In each dataset, H0:mu_{yes} = mu_{no} is rejected --> 
# centroids of Private and non-private schools differ significantly

#test the difference between centroids
boxm_results <- testlist %>% map(function(x) boxM_test(x, "Private"))

#LDA
lda_results <- map2(testlist, names(testlist), 
                    function(x,y){lda_analysis(x,"Private", y)}) %>%
  do.call("rbind", .)
  
#QDA
qda_results <- map2(testlist, names(testlist), 
                    function(x,y){qda_analysis(x,"Private", y)}) %>%
  do.call("rbind", .)
  
#KNN
knn_output <- map2(testlist, names(testlist), 
                    function(x,y){knn_analysis(x,"Private", y, 1:100)})
knn_results <- lapply(knn_output, function(element){
  element[[2]] # The first element contains graphs
})
knn_results <- knn_results %>%
  do.call("rbind", .)

#bagging
bagging_output <- map2(testlist, names(testlist), 
                    function(x,y){bagging_analysis(x, "Private", y, 
                                              ntree = 1000)})
bagging_results <- lapply(bagging_output, function(element){
  element[[2]] # The first element contains graphs
})
bagging_results <- bagging_results %>%
  do.call("rbind", .)

#random forests
rf_output <- map2(testlist, names(testlist), 
                    function(x,y){RF_analysis(x, "Private", y, 
                                              ntree = 1000, mtry = 10)})
rf_results <- lapply(rf_output, function(element){
  element[[2]] # The first element contains graphs
})
rf_results <- rf_results %>%
  do.call("rbind", .)
```

```{r compareresults, echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(lda_results) %>% 
  bind_rows(as.data.frame(qda_results)) %>%
  bind_rows(as.data.frame(knn_results)) %>%
  bind_rows(as.data.frame(bagging_results)) %>%
  bind_rows(as.data.frame(rf_results)) %>%
  dplyr::select(-K) %>%
  pivot_wider(names_from = data, 
              values_from = c(hitrate, sensitivity, specificity)) -> A
A %>% knitr::kable(
  caption = 'A performance comparison of the different methods',
  digits = 3,
  col.names = c("Method",rep(c("Centered", "Centered Logtransf",
                   "Standardized", "Standardized Logtransf"), 3)),
  booktabs = TRUE, 
  valign = 't'
) %>%
  row_spec(0, angle = 90) %>% 
  add_header_above(c(" ", "Hit rate" = 4, "Sensitivity" = 4, "Specificity" = 4)) %>%
  column_spec(2, color = "white", 
              background = spec_color(unlist(A[,2]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(3, color = "white", 
              background = spec_color(unlist(A[,3]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(4, color = "white", 
              background = spec_color(unlist(A[,4]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(5, color = "white", 
              background = spec_color(unlist(A[,5]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(6, color = "white", 
              background = spec_color(unlist(A[,6]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(7, color = "white", 
              background = spec_color(unlist(A[,7]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(8, color = "white", 
              background = spec_color(unlist(A[,8]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(9, color = "white", 
              background = spec_color(unlist(A[,9]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(10, color = "white", 
              background = spec_color(unlist(A[,10]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(11, color = "white", 
              background = spec_color(unlist(A[,11]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(12, color = "white", 
              background = spec_color(unlist(A[,12]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) %>%
  column_spec(13, color = "white", 
              background = spec_color(unlist(A[,13]), option = "viridis", 
                                      direction = -1, 
                                      scale_from = c(min(A[,2:13]), 
                                                     max(A[,2:13])))) 
  

```



For the K-nearest neighbour (KNN) approach, the function knn_cv from the *class* package is used to execute KNN with LOOCV and test values of K between 1 and 100. The model with the highest hit rate is chosen as the final model in the table. If there are multiple models with the same hit rate, the smallest K is chosen, Figure \@ref(fig:KNNgraphs) shows the results in each of the datasets.

```{r KNNgraphs, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, fig.width = 8, fig.height=5, fig.cap="Evolution of hit rate, sensitivity and specificity for different K in the KNN approach. A shows the centered data, B the centered data with log-transformed skewed variables, C shows the standardized data, and D the standardized data with log-transformed skewed variables."}
knn_graphs <- lapply(knn_output, function(element){
  element[[1]] # The first element contains graphs
  })
knn_graphs[[1]] + knn_graphs[[2]] + knn_graphs[[3]] + knn_graphs[[4]] +
  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")
```


```{r bagginggraphs, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, fig.width = 8, fig.height=5, fig.cap="Variance importance plots for the bagging models. A shows the centered data, B the centered data with log-transformed skewed variables, C shows the standardized data, and D the standardized data with log-transformed skewed variables."}
bagging_graphs <- lapply(bagging_output, function(element){
  element[[1]] # The first element contains graphs
  })
bagging_graphs[[1]] + bagging_graphs[[2]] + bagging_graphs[[3]] + bagging_graphs[[4]] +
  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")
```

```{r rfgraphs, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, fig.width = 8, fig.height=5, fig.cap="Variance importance plots for the random forest models. A shows the centered data, B the centered data with log-transformed skewed variables, C shows the standardized data, and D the standardized data with log-transformed skewed variables."}
rf_graphs <- lapply(rf_output, function(element){
  element[[1]] # The first element contains graphs
  })
rf_graphs[[1]] + rf_graphs[[2]] + rf_graphs[[3]] + rf_graphs[[4]] +
  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")
```

# Task 2

```{r load2, include=FALSE, message=FALSE, warning=FALSE}
load(find_root_file("data/fashion.Rdata", 
     criterion = has_file("MultivariateStatistics_assignment.Rproj")))
train.data <- train.data %>% scale(center = TRUE, scale = FALSE)
```

# Task 3
```{r load3, include=FALSE, message=FALSE, warning=FALSE}
if (file.exists(find_root_file("data/rect_constr.csv", 
     criterion = has_file("MultivariateStatistics_assignment.Rproj")))) {
  rect_constr <- read.csv(
    find_root_file("data/rect_constr.csv", 
                   criterion = 
                     has_file("MultivariateStatistics_assignment.Rproj")))
} else {
  data(rect_constr) 
}
if (file.exists(find_root_file("data/rectangles.csv", 
     criterion = has_file("MultivariateStatistics_assignment.Rproj")))) {
  rectangles <- read.csv(
    find_root_file("data/rectangles.csv", 
                   criterion = 
                     has_file("MultivariateStatistics_assignment.Rproj")))
} else {
  data(rectangles) 
}
```

# Appendix

